{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS\n",
    "DATA_TRAIN_PATH = \"./../data/raw/train.csv\"\n",
    "DATA_TEST_PATH = \"./../data/raw/test.csv\" \n",
    "OUTPUT_PATH = './../results/predictions/oct2_paola.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('./../src/')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *\n",
    "from utils import *\n",
    "from pipeline import * \n",
    "from proj1_helpers import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = np.where(tX[:,22]==0)\n",
    "tX1 = tX[inds] \n",
    "y1 = y[inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./../src/pipeline.py:37: RuntimeWarning: Mean of empty slice\n",
      "  col_mean = np.nanmean(tX, axis=1)\n",
      "/usr/lib/python3.8/site-packages/numpy/lib/nanfunctions.py:1664: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    }
   ],
   "source": [
    "# Simple division\n",
    "# TODO: Cross validation\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(tX1, y1, 0.1, myseed=1)\n",
    "X_train, X_test = preprocessing(X_train, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.85      0.92      0.89     66940\n",
      "         1.0       0.70      0.54      0.61     22982\n",
      "\n",
      "    accuracy                           0.82     89922\n",
      "   macro avg       0.78      0.73      0.75     89922\n",
      "weighted avg       0.81      0.82      0.82     89922\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter = 700)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./../src/pipeline.py:37: RuntimeWarning: Mean of empty slice\n",
      "  col_mean = np.nanmean(tX, axis=1)\n",
      "/usr/lib/python3.8/site-packages/numpy/lib/nanfunctions.py:1664: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.85      0.92      0.89     66940\n",
      "         1.0       0.70      0.54      0.61     22982\n",
      "\n",
      "    accuracy                           0.82     89922\n",
      "   macro avg       0.78      0.73      0.75     89922\n",
      "weighted avg       0.81      0.82      0.82     89922\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.86      0.94      0.90     66940\n",
      "         1.0       0.76      0.57      0.65     22982\n",
      "\n",
      "    accuracy                           0.84     89922\n",
      "   macro avg       0.81      0.75      0.78     89922\n",
      "weighted avg       0.84      0.84      0.84     89922\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inds = np.where(tX[:,22]==0)\n",
    "tX1 = tX[inds] \n",
    "y1 = y[inds]\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(tX1, y1, 0.1, myseed=1)\n",
    "X_train, X_test = preprocessing(X_train, X_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter = 700)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./../src/pipeline.py:37: RuntimeWarning: Mean of empty slice\n",
      "  col_mean = np.nanmean(tX, axis=1)\n",
      "/usr/lib/python3.8/site-packages/numpy/lib/nanfunctions.py:1664: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.75      0.83      0.79     44882\n",
      "         1.0       0.62      0.49      0.55     24908\n",
      "\n",
      "    accuracy                           0.71     69790\n",
      "   macro avg       0.68      0.66      0.67     69790\n",
      "weighted avg       0.70      0.71      0.70     69790\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.82      0.89      0.85     44882\n",
      "         1.0       0.77      0.65      0.71     24908\n",
      "\n",
      "    accuracy                           0.81     69790\n",
      "   macro avg       0.79      0.77      0.78     69790\n",
      "weighted avg       0.80      0.81      0.80     69790\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inds = np.where(tX[:,22]==1)\n",
    "tX1 = tX[inds] \n",
    "y1 = y[inds]\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(tX1, y1, 0.1, myseed=1)\n",
    "X_train, X_test = preprocessing(X_train, X_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter = 700)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.73      0.74      0.74     22191\n",
      "         1.0       0.75      0.73      0.74     23151\n",
      "\n",
      "    accuracy                           0.74     45342\n",
      "   macro avg       0.74      0.74      0.74     45342\n",
      "weighted avg       0.74      0.74      0.74     45342\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.83      0.83      0.83     22191\n",
      "         1.0       0.84      0.84      0.84     23151\n",
      "\n",
      "    accuracy                           0.84     45342\n",
      "   macro avg       0.84      0.84      0.84     45342\n",
      "weighted avg       0.84      0.84      0.84     45342\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inds = np.where(tX[:,22]==2)\n",
    "tX1 = tX[inds] \n",
    "y1 = y[inds]\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(tX1, y1, 0.1, myseed=1)\n",
    "X_train, X_test = preprocessing(X_train, X_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter = 700)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.76      0.89      0.82     13918\n",
      "         1.0       0.59      0.37      0.45      6030\n",
      "\n",
      "    accuracy                           0.73     19948\n",
      "   macro avg       0.68      0.63      0.64     19948\n",
      "weighted avg       0.71      0.73      0.71     19948\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.85      0.92      0.88     13918\n",
      "         1.0       0.77      0.62      0.69      6030\n",
      "\n",
      "    accuracy                           0.83     19948\n",
      "   macro avg       0.81      0.77      0.79     19948\n",
      "weighted avg       0.83      0.83      0.82     19948\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inds = np.where(tX[:,22]==3)\n",
    "tX1 = tX[inds] \n",
    "y1 = y[inds]\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(tX1, y1, 0.1, myseed=1)\n",
    "X_train, X_test = preprocessing(X_train, X_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter = 700)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "important_features = pd.Series(data=clf.feature_importances_,index=x_dummies.columns)\n",
    "important_features.sort_values(ascending=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.85      0.91      0.88    147877\n",
      "         1.0       0.79      0.69      0.74     77123\n",
      "\n",
      "    accuracy                           0.83    225000\n",
      "   macro avg       0.82      0.80      0.81    225000\n",
      "weighted avg       0.83      0.83      0.83    225000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "inds = np.where(tX[:,22]>-1)\n",
    "tX1 = tX[inds] \n",
    "y1 = y[inds]\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(tX1, y1, 0.1, myseed=1)\n",
    "X_train, X_test = preprocessing(X_train, X_test)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.78      0.86      0.82    147877\n",
      "         1.0       0.67      0.54      0.60     77123\n",
      "\n",
      "    accuracy                           0.75    225000\n",
      "   macro avg       0.73      0.70      0.71    225000\n",
      "weighted avg       0.74      0.75      0.74    225000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter = 700)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['DER_mass_MMC', 'DER_mass_transverse_met_lep',\n",
    "       'DER_mass_vis', 'DER_pt_h', 'DER_deltaeta_jet_jet', 'DER_mass_jet_jet',\n",
    "       'DER_prodeta_jet_jet', 'DER_deltar_tau_lep', 'DER_pt_tot', 'DER_sum_pt',\n",
    "       'DER_pt_ratio_lep_tau', 'DER_met_phi_centrality',\n",
    "       'DER_lep_eta_centrality', 'PRI_tau_pt', 'PRI_tau_eta', 'PRI_tau_phi',\n",
    "       'PRI_lep_pt', 'PRI_lep_eta', 'PRI_lep_phi', 'PRI_met', 'PRI_met_phi',\n",
    "       'PRI_met_sumet', 'PRI_jet_num', 'PRI_jet_leading_pt',\n",
    "       'PRI_jet_leading_eta', 'PRI_jet_leading_phi', 'PRI_jet_subleading_pt',\n",
    "       'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi', 'PRI_jet_all_pt']\n",
    "\n",
    "\n",
    "col_names_imputed = [\"imp_\" + x  for x in col_names]\n",
    "all_col_names = col_names + col_names_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "important_features = pd.Series(data=clf.feature_importances_,index=all_col_names)\n",
    "important_features.sort_values(ascending=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DER_mass_MMC                       0.130940\n",
       "DER_mass_transverse_met_lep        0.089455\n",
       "DER_mass_vis                       0.076703\n",
       "PRI_tau_pt                         0.057833\n",
       "DER_met_phi_centrality             0.052825\n",
       "DER_pt_ratio_lep_tau               0.048389\n",
       "DER_deltar_tau_lep                 0.044895\n",
       "PRI_met                            0.041150\n",
       "DER_pt_h                           0.033708\n",
       "DER_sum_pt                         0.033298\n",
       "PRI_lep_pt                         0.030197\n",
       "DER_pt_tot                         0.029408\n",
       "PRI_met_sumet                      0.028459\n",
       "PRI_lep_eta                        0.027692\n",
       "PRI_tau_eta                        0.026922\n",
       "PRI_lep_phi                        0.025357\n",
       "PRI_tau_phi                        0.025239\n",
       "PRI_met_phi                        0.024952\n",
       "PRI_jet_leading_eta                0.020959\n",
       "PRI_jet_all_pt                     0.017869\n",
       "PRI_jet_leading_pt                 0.016299\n",
       "DER_lep_eta_centrality             0.016002\n",
       "DER_prodeta_jet_jet                0.015991\n",
       "PRI_jet_leading_phi                0.015782\n",
       "DER_mass_jet_jet                   0.012012\n",
       "DER_deltaeta_jet_jet               0.011633\n",
       "imp_DER_mass_MMC                   0.011097\n",
       "PRI_jet_subleading_eta             0.007247\n",
       "PRI_jet_subleading_pt              0.006851\n",
       "PRI_jet_subleading_phi             0.006396\n",
       "PRI_jet_num                        0.004706\n",
       "imp_PRI_jet_leading_eta            0.001416\n",
       "imp_PRI_jet_subleading_phi         0.001327\n",
       "imp_PRI_jet_leading_phi            0.001294\n",
       "imp_PRI_jet_leading_pt             0.001064\n",
       "imp_DER_lep_eta_centrality         0.001043\n",
       "imp_DER_prodeta_jet_jet            0.000905\n",
       "imp_PRI_jet_subleading_pt          0.000807\n",
       "imp_DER_mass_jet_jet               0.000769\n",
       "imp_PRI_jet_subleading_eta         0.000618\n",
       "imp_DER_deltaeta_jet_jet           0.000491\n",
       "imp_PRI_met                        0.000000\n",
       "imp_PRI_jet_num                    0.000000\n",
       "imp_PRI_met_sumet                  0.000000\n",
       "imp_PRI_met_phi                    0.000000\n",
       "imp_DER_sum_pt                     0.000000\n",
       "imp_PRI_tau_pt                     0.000000\n",
       "imp_PRI_lep_phi                    0.000000\n",
       "imp_PRI_lep_eta                    0.000000\n",
       "imp_PRI_lep_pt                     0.000000\n",
       "imp_PRI_tau_phi                    0.000000\n",
       "imp_PRI_tau_eta                    0.000000\n",
       "imp_DER_pt_tot                     0.000000\n",
       "imp_DER_met_phi_centrality         0.000000\n",
       "imp_DER_pt_ratio_lep_tau           0.000000\n",
       "imp_DER_mass_transverse_met_lep    0.000000\n",
       "imp_DER_mass_vis                   0.000000\n",
       "imp_DER_pt_h                       0.000000\n",
       "imp_DER_deltar_tau_lep             0.000000\n",
       "imp_PRI_jet_all_pt                 0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=17328.67951399868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./../src/implementations.py:50: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1 + np.exp(-t))\n",
      "./../src/implementations.py:56: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = y.T.dot(np.log(pred)) + (1 - y).T.dot(np.log(1 - pred))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=nan\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=0, loss=nan\n",
      "Current iteration=0, loss=nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-49cabce3bb83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlambdas_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgammas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/MLproject1/src/implementations.py\u001b[0m in \u001b[0;36mreg_logistic_regression\u001b[0;34m(y, tx, lambda_, initial_w, max_iter, gamma)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;31m# get loss and update w.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_by_penalized_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;31m# log info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/MLproject1/src/implementations.py\u001b[0m in \u001b[0;36mlearning_by_penalized_gradient\u001b[0;34m(y, tx, w, gamma, lambda_)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mupdated\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \"\"\"\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpenalized_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/MLproject1/src/implementations.py\u001b[0m in \u001b[0;36mpenalized_logistic_regression\u001b[0;34m(y, tx, w, lambda_)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_loss_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_gradient_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/MLproject1/src/implementations.py\u001b[0m in \u001b[0;36mcalculate_gradient_log\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;34m\"\"\"compute the gradient of loss.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ws = []\n",
    "losses = []\n",
    "\n",
    "#Gridsearch for lambda_\n",
    "w = np.zeros(X_train.shape[1])\n",
    "gammas = [0.01,0.1,0.2]\n",
    "lambdas_ = [0.0001,0.001,0.01, 0.05,0.1, 0.2, 0.5, 1]\n",
    "for l in lambdas_:\n",
    "    for g in gammas:\n",
    "        w, loss = reg_logistic_regression(y_train, X_train, l,w,100,g)\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
